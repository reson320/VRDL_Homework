{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Faster R-CNN Training/Validation/Testing on SVHN for Digit Recognition (Google Colab)\n",
        "Task 1: BBox and Class (category_id) prediction\n",
        "Task 2: Full number sequence recognition (post-processing)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "try:\n",
        "    from torchvision.transforms.v2 import functional as F, ToTensor, Compose\n",
        "    using_v2_transforms = True\n",
        "except ImportError:\n",
        "    print(\"torchvision.transforms.v2 not found, falling back to v1 transforms.\")\n",
        "    print(\"Consider upgrading torchvision: pip install --upgrade torchvision\")\n",
        "    from torchvision import transforms as T # Fallback to v1\n",
        "    using_v2_transforms = False\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "\n",
        "try:\n",
        "    from pycocotools.coco import COCO\n",
        "except ImportError:\n",
        "    print(\"pycocotools not found. Installing...\")\n",
        "    !pip install pycocotools --quiet\n",
        "    from pycocotools.coco import COCO\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import zipfile\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/SVHN_dataset.zip'\n",
        "BASE_DIR = 'data/your_dataset'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(BASE_DIR)\n",
        "\n",
        "\n",
        "\n",
        "TRAIN_IMG_DIR = os.path.join(BASE_DIR, 'train')\n",
        "VAL_IMG_DIR = os.path.join(BASE_DIR, 'valid')\n",
        "TEST_IMG_DIR = os.path.join(BASE_DIR, 'test')\n",
        "\n",
        "TRAIN_ANNOTATION_PATH = os.path.join(BASE_DIR, 'train.json')\n",
        "VAL_ANNOTATION_PATH = os.path.join(BASE_DIR, 'valid.json')\n",
        "TEST_ANNOTATION_PATH = os.path.join(BASE_DIR, 'test.json')\n",
        "\n",
        "# --- Output Files ---\n",
        "TASK1_OUTPUT_PATH = 'pred.json' # Task 1 輸出檔名 (BBox & Class)\n",
        "TASK2_OUTPUT_PATH = 'pred.csv' # Task 2 輸出檔名 (Number Sequence)\n",
        "MODEL_SAVE_PATH = 'fasterrcnn_svhn_best_model.pth'\n",
        "\n",
        "# --- 模型與訓練參數 ---\n",
        "# Category ID Mapping:\n",
        "# 1: '0', 2: '1', 3: '2', 4: '3', 5: '4',\n",
        "# 6: '5', 7: '6', 8: '7', 9: '8', 10: '9'\n",
        "NUM_CLASSES = 11  # 10 個數字 + 1 個背景\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 0.0005\n",
        "MOMENTUM = 0.9\n",
        "LR_STEP_SIZE = 1\n",
        "LR_GAMMA = 0.1\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "PRINT_FREQ = 100\n",
        "BEST_VAL_LOSS = float('inf')\n",
        "\n",
        "# --- Task 2 Parameters ---\n",
        "TASK2_SCORE_THRESHOLD = 0.5 # THRESHOLD for Task 2\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "print(f\"Using torchvision v2 transforms: {using_v2_transforms}\")\n",
        "\n",
        "\n",
        "\n",
        "# --- 2. 資料集類別 (SVHNDataset) ---\n",
        "class SVHNDataset(Dataset):\n",
        "    def __init__(self, img_dir, annotation_path, transforms=None, is_test=False):\n",
        "        self.img_dir = img_dir\n",
        "        self.coco = COCO(annotation_path)\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "        self.transforms = transforms\n",
        "        self.is_test = is_test\n",
        "        print(f\"Loaded {len(self.ids)} images from {annotation_path}. Is test set: {self.is_test}\")\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        coco = self.coco\n",
        "        img_id = self.ids[index]\n",
        "        img_info = coco.loadImgs(img_id)[0]\n",
        "        img_path = os.path.join(self.img_dir, img_info['file_name'])\n",
        "\n",
        "        try:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Image file not found at {img_path}\")\n",
        "            return None\n",
        "\n",
        "        target = {}\n",
        "        target[\"image_id\"] = torch.tensor([img_id])\n",
        "\n",
        "        if not self.is_test:\n",
        "            ann_ids = coco.getAnnIds(imgIds=img_id)\n",
        "            coco_anns = coco.loadAnns(ann_ids)\n",
        "\n",
        "            boxes = []\n",
        "            labels = []\n",
        "            areas = []\n",
        "            iscrowd = []\n",
        "\n",
        "            for ann in coco_anns:\n",
        "                xmin, ymin, w, h = ann['bbox']\n",
        "                xmax = xmin + w\n",
        "                ymax = ymin + h\n",
        "\n",
        "                img_w, img_h = img.size\n",
        "                xmin = max(0, xmin)\n",
        "                ymin = max(0, ymin)\n",
        "                xmax = min(img_w, xmax)\n",
        "                ymax = min(img_h, ymax)\n",
        "\n",
        "                if xmax <= xmin or ymax <= ymin:\n",
        "                    continue\n",
        "\n",
        "                boxes.append([xmin, ymin, xmax, ymax])\n",
        "                labels.append(ann['category_id'])\n",
        "                areas.append(ann.get('area', w * h))\n",
        "                iscrowd.append(ann.get('iscrowd', 0))\n",
        "\n",
        "            target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32) if boxes else torch.empty((0, 4), dtype=torch.float32)\n",
        "            target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) if labels else torch.empty((0,), dtype=torch.int64)\n",
        "            target[\"area\"] = torch.as_tensor(areas, dtype=torch.float32) if areas else torch.empty((0,), dtype=torch.float32)\n",
        "            target[\"iscrowd\"] = torch.as_tensor(iscrowd, dtype=torch.int64) if iscrowd else torch.empty((0,), dtype=torch.int64)\n",
        "        else:\n",
        "            target[\"boxes\"] = torch.empty((0, 4), dtype=torch.float32)\n",
        "            target[\"labels\"] = torch.empty((0,), dtype=torch.int64)\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            if using_v2_transforms:\n",
        "                img, target = self.transforms(img, target)\n",
        "            else:\n",
        "                img = self.transforms(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "\n",
        "# --- 3. 資料轉換與 DataLoader ---\n",
        "def get_transform(train):\n",
        "    if using_v2_transforms:\n",
        "        transforms = []\n",
        "        transforms.append(ToTensor())\n",
        "        return Compose(transforms)\n",
        "    else: # v1 transforms\n",
        "        transforms = []\n",
        "        transforms.append(T.ToTensor())\n",
        "        return T.Compose(transforms)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = list(filter(lambda x: x is not None, batch))\n",
        "    if not batch:\n",
        "        return None, None\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "print(\"Creating Datasets...\")\n",
        "dataset_train = SVHNDataset(TRAIN_IMG_DIR, TRAIN_ANNOTATION_PATH, transforms=get_transform(train=True))\n",
        "dataset_val = SVHNDataset(VAL_IMG_DIR, VAL_ANNOTATION_PATH, transforms=get_transform(train=False))\n",
        "dataset_test = SVHNDataset(TEST_IMG_DIR, TEST_ANNOTATION_PATH, transforms=get_transform(train=False), is_test=True)\n",
        "\n",
        "print(\"Creating DataLoaders...\")\n",
        "data_loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=12, collate_fn=collate_fn, pin_memory=True)\n",
        "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, num_workers=12, collate_fn=collate_fn, pin_memory=True)\n",
        "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, num_workers=12, collate_fn=collate_fn, pin_memory=True)\n",
        "print(\"DataLoaders created.\")\n",
        "\n",
        "# --- 4. 模型定義 ---\n",
        "def get_model(num_classes):\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "print(\"Loading model...\")\n",
        "model = get_model(NUM_CLASSES)\n",
        "model.to(DEVICE)\n",
        "print(\"Model loaded.\")\n",
        "\n",
        "# 載入已儲存的模型權重\n",
        "if os.path.exists(MODEL_SAVE_PATH):\n",
        "    model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
        "    print(f\"Model weights loaded from {MODEL_SAVE_PATH}\")\n",
        "else:\n",
        "    print(\"Warning: Pretrained weights not found. Training from scratch.\")\n",
        "\n",
        "print(\"Model ready.\")\n",
        "\n",
        "# --- 5. 優化器與學習率調整器 ---\n",
        "print(\"Setting up optimizer and scheduler...\")\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
        "print(\"Optimizer and scheduler ready.\")\n",
        "\n",
        "# --- 6. 訓練與驗證 ---\n",
        "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
        "start_time_train = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    start_time_epoch = time.time()\n",
        "\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_batch_count = 0\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} --- Training ---\")\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, batch_data in enumerate(data_loader_train):\n",
        "\n",
        "\n",
        "        if batch_data is None or batch_data[0] is None or batch_data[1] is None:\n",
        "            print(f\"Warning: Skipping empty or invalid batch at training iteration {i}.\")\n",
        "            continue\n",
        "\n",
        "        images, targets = batch_data\n",
        "\n",
        "\n",
        "        if not images or not targets:\n",
        "             print(f\"Warning: Skipping training batch {i} due to empty images or targets after collate_fn.\")\n",
        "             continue\n",
        "\n",
        "\n",
        "        try:\n",
        "            images = list(image.to(DEVICE) for image in images)\n",
        "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "        except Exception as e:\n",
        "            print(f\"Error moving batch {i} data to device {DEVICE}: {e}\")\n",
        "            continue\n",
        "\n",
        "\n",
        "\n",
        "        try:\n",
        "            # 前向傳播\n",
        "            loss_dict = model(images, targets)\n",
        "\n",
        "            # 計算總損失\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            loss_value = losses.item() # .item() 將 tensor 轉為 python float\n",
        "\n",
        "            # 檢查損失是否有效\n",
        "            if not np.isfinite(loss_value):\n",
        "                 print(f\"\\nWarning: Non-finite loss detected during training: {loss_value}. Skipping batch {i}.\")\n",
        "                 print(\"Loss dict:\", {k: v.item() if torch.is_tensor(v) else v for k, v in loss_dict.items()})\n",
        "                 # 清除梯度並跳過優化步驟\n",
        "                 optimizer.zero_grad()\n",
        "                 continue\n",
        "\n",
        "            train_loss += loss_value\n",
        "            train_batch_count += 1\n",
        "\n",
        "            # 反向傳播與優化\n",
        "            losses.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if (i + 1) % PRINT_FREQ == 0 or i == len(data_loader_train) - 1:\n",
        "                current_avg_loss = train_loss / train_batch_count if train_batch_count > 0 else 0\n",
        "                print(f\"Batch [{i+1}/{len(data_loader_train)}], Current Avg Loss: {current_avg_loss:.4f} (Last Batch Loss: {loss_value:.4f})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError during training batch {i}: {e}\")\n",
        "            print(\"Image IDs in this batch:\", [t['image_id'].item() for t in targets if 'image_id' in t])\n",
        "            optimizer.zero_grad()\n",
        "            continue\n",
        "\n",
        "\n",
        "    # 計算 Epoch 平均訓練損失\n",
        "    avg_train_loss = train_loss / train_batch_count if train_batch_count > 0 else 0\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    val_loss = 0\n",
        "    val_batch_count = 0\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} --- Validation ---\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.train()\n",
        "        for i, batch_data in enumerate(data_loader_val):\n",
        "            if batch_data is None or batch_data[0] is None or batch_data[1] is None:\n",
        "                continue\n",
        "\n",
        "            images, targets = batch_data\n",
        "            #print(f\"[Debug] Batch {i}: type(targets)={type(targets)}, type(targets[0])={type(targets[0])}\")\n",
        "            # 若 targets 是 list 且裡面還是 list，解包\n",
        "            while isinstance(targets, list) and len(targets) > 0 and isinstance(targets[0], list):\n",
        "                targets = targets[0]\n",
        "\n",
        "            if not images or not targets or not isinstance(targets[0], dict):\n",
        "                print(f\"=Skipping batch {i}: targets are not valid.\")\n",
        "                continue\n",
        "\n",
        "\n",
        "            if not images or not targets:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                images = [img.to(DEVICE) for img in images]\n",
        "                targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "            except Exception as e:\n",
        "                print(f\"Error moving validation batch {i} data to device {DEVICE}: {e}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                loss_dict = model(images, targets)\n",
        "\n",
        "                if not isinstance(loss_dict, dict):\n",
        "                    raise TypeError(f\"Expected dict, got {type(loss_dict)}\")\n",
        "\n",
        "                losses = sum(loss for loss in loss_dict.values())\n",
        "                loss_value = losses.item()\n",
        "\n",
        "                if not np.isfinite(loss_value):\n",
        "                    print(f\"\\nWarning: Non-finite validation loss detected: {loss_value} at batch {i}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                val_loss += loss_value\n",
        "                val_batch_count += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError during validation batch {i}: {e}\")\n",
        "                print(\"Image IDs in this batch:\", [t['image_id'].item() for t in targets if 'image_id' in t])\n",
        "                continue\n",
        "\n",
        "\n",
        "    # 計算 Epoch 平均驗證損失\n",
        "    # 避免 val_batch_count 為 0 的情況\n",
        "    avg_val_loss = val_loss / val_batch_count if val_batch_count > 0 else float('inf')\n",
        "\n",
        "    # --- Epoch Summary & Model Saving ---\n",
        "    end_time_epoch = time.time()\n",
        "    elapsed_time = end_time_epoch - start_time_epoch\n",
        "    print(f\"\\n--- Epoch {epoch+1} Summary ---\")\n",
        "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    if val_batch_count > 0:\n",
        "        print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n",
        "    else:\n",
        "        print(\"Validation Loss: N/A (No valid validation batches processed)\")\n",
        "    print(f\"Time Elapsed: {elapsed_time:.2f} seconds\")\n",
        "    print(f\"Current Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "    # 只有在驗證損失有效且改善時才保存模型\n",
        "    if val_batch_count > 0 and avg_val_loss < BEST_VAL_LOSS:\n",
        "        BEST_VAL_LOSS = avg_val_loss\n",
        "        try:\n",
        "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "            print(f\"Validation loss improved to {avg_val_loss:.4f}. Model saved to {MODEL_SAVE_PATH}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving model: {e}\")\n",
        "    elif val_batch_count > 0:\n",
        "        print(f\"Validation loss ({avg_val_loss:.4f}) did not improve from best ({BEST_VAL_LOSS:.4f}).\")\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # --- 更新學習率 ---\n",
        "\n",
        "    lr_scheduler.step(avg_val_loss)\n",
        "\n",
        "    #下載目前 epoch model（僅在模型檔存在時下載）\n",
        "    '''\n",
        "    from google.colab import files\n",
        "    if os.path.exists(MODEL_SAVE_PATH):\n",
        "        files.download(MODEL_SAVE_PATH)\n",
        "    else:\n",
        "        print(f\"Warning: Skipped download. Model file {MODEL_SAVE_PATH} does not exist.\")\n",
        "    '''\n",
        "\n",
        "# --- 訓練結束 ---\n",
        "total_training_time = time.time() - start_time_train\n",
        "print(f\"\\n--- Training Finished ---\")\n",
        "print(f\"Total Training Time: {total_training_time:.2f} seconds ({total_training_time/60:.2f} minutes)\")\n",
        "print(f\"Best Validation Loss Achieved: {BEST_VAL_LOSS:.4f}\")\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "# --- 載入最佳模型進行最終預測 ---\n",
        "print(\"\\n--- Loading best model for final prediction ---\")\n",
        "# 檢查最佳模型文件是否存在\n",
        "if os.path.exists(MODEL_SAVE_PATH):\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
        "        print(f\"Loaded best model from {MODEL_SAVE_PATH} with validation loss {BEST_VAL_LOSS:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not load best model from {MODEL_SAVE_PATH}: {e}. Using model from the last epoch.\")\n",
        "else:\n",
        "    print(f\"Warning: Best model file {MODEL_SAVE_PATH} not found. Using model from the last epoch.\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# --- 7. 生成 Task 1 預測 (使用測試集) ---\n",
        "print(f\"\\n--- Generating Task 1 predictions ({TASK1_OUTPUT_PATH}) ---\")\n",
        "task1_results = []\n",
        "with torch.no_grad():\n",
        "    for i, batch_data in enumerate(data_loader_test):\n",
        "        if batch_data is None or batch_data[0] is None: continue\n",
        "        images, targets = batch_data\n",
        "        images = list(img.to(DEVICE) for img in images)\n",
        "        if not targets or targets[0] is None or 'image_id' not in targets[0]: continue\n",
        "        original_image_id = targets[0]['image_id'].item()\n",
        "\n",
        "        if (i+1) % 1000 == 0:\n",
        "             print(f\"Task 1: Processing test image {i+1}/{len(data_loader_test)}\")\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        for output in outputs:\n",
        "            boxes = output['boxes'].cpu().numpy()\n",
        "            labels = output['labels'].cpu().numpy() # category_id (1-10)\n",
        "            scores = output['scores'].cpu().numpy()\n",
        "\n",
        "            for box, label, score in zip(boxes, labels, scores):\n",
        "                xmin, ymin, xmax, ymax = box\n",
        "                width = xmax - xmin\n",
        "                height = ymax - ymin\n",
        "                if width <= 0 or height <= 0: continue\n",
        "\n",
        "                prediction = {\n",
        "                    \"image_id\": original_image_id,\n",
        "                    \"bbox\": [float(xmin), float(ymin), float(width), float(height)],\n",
        "                    \"score\": float(score),\n",
        "                    \"category_id\": int(label) # Class prediction\n",
        "                }\n",
        "                task1_results.append(prediction)\n",
        "\n",
        "# --- 8. 保存 Task 1 預測結果 ---\n",
        "print(f\"\\nSaving Task 1 predictions to {TASK1_OUTPUT_PATH}...\")\n",
        "with open(TASK1_OUTPUT_PATH, 'w') as f:\n",
        "    json.dump(task1_results, f, indent=4)\n",
        "print(\"Task 1 prediction file saved successfully.\")\n",
        "\n",
        "\n",
        "# --- 9. 生成 Task 2 預測 (後處理 Task 1 結果) ---\n",
        "print(f\"\\n--- Generating Task 2 predictions ({TASK2_OUTPUT_PATH}) ---\")\n",
        "\n",
        "# Category ID 到實際數字字元的映射\n",
        "category_id_to_digit = {\n",
        "    1: '0', 2: '1', 3: '2', 4: '3', 5: '4',\n",
        "    6: '5', 7: '6', 8: '7', 9: '8', 10: '9'\n",
        "}\n",
        "\n",
        "# 按 image_id 分組 Task 1 的結果\n",
        "detections_by_image = defaultdict(list)\n",
        "for det in task1_results:\n",
        "    detections_by_image[det['image_id']].append(det)\n",
        "\n",
        "task2_results = {} # 儲存 Task 2 結果 {image_id: number_string}\n",
        "\n",
        "print(f\"Processing Task 1 results for Task 2 (Score Threshold: {TASK2_SCORE_THRESHOLD})...\")\n",
        "processed_images = 0\n",
        "for image_id, detections in detections_by_image.items():\n",
        "    # 1. 過濾低信心結果\n",
        "    high_conf_detections = [d for d in detections if d['score'] >= TASK2_SCORE_THRESHOLD]\n",
        "\n",
        "    if not high_conf_detections:\n",
        "        task2_results[image_id] = \"-1\" # 如果沒有高信度結果，則-1\n",
        "        continue\n",
        "\n",
        "    # 2. 根據 BBox 的 x 座標排序 (使用 BBox 中心點的 x 座標)\n",
        "    # bbox: [xmin, ymin, width, height]\n",
        "    # center_x = xmin + width / 2\n",
        "    sorted_detections = sorted(\n",
        "        high_conf_detections,\n",
        "        key=lambda d: d['bbox'][0] + d['bbox'][2] / 2\n",
        "    )\n",
        "\n",
        "    # 3. 轉換 category_id 並拼接字串\n",
        "    number_sequence = \"\"\n",
        "    for d in sorted_detections:\n",
        "        digit = category_id_to_digit.get(d['category_id'])\n",
        "        if digit is not None:\n",
        "            number_sequence += digit\n",
        "        else:\n",
        "            print(f\"Warning: Unknown category_id {d['category_id']} found for image {image_id}. Skipping.\")\n",
        "\n",
        "    task2_results[image_id] = number_sequence\n",
        "    processed_images += 1\n",
        "    if processed_images % 1000 == 0:\n",
        "         print(f\"Task 2: Processed detections for {processed_images} images...\")\n",
        "\n",
        "print(f\"Task 2 processing finished for {processed_images} images.\")\n",
        "with open(TEST_ANNOTATION_PATH) as f:\n",
        "    test_json = json.load(f)\n",
        "all_image_ids = set(img['id'] for img in test_json['images'])\n",
        "missing_ids = all_image_ids - set(task2_results.keys())\n",
        "for missing_id in missing_ids:\n",
        "    task2_results[missing_id] = \"-1\"\n",
        "    print(f\"Added missing image_id {missing_id} with prediction -1\")\n",
        "\n",
        "print(f\"Final total task2 results: {len(task2_results)} (should be {len(all_image_ids)})\")\n",
        "\n",
        "# --- 10. 保存 Task 2 預測結果 ---\n",
        "print(f\"\\nSaving Task 2 predictions to {TASK2_OUTPUT_PATH}...\")\n",
        "\n",
        "import csv\n",
        "\n",
        "with open('pred.csv', 'w', newline='', encoding='utf-8-sig') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['image_id', 'pred_label'])\n",
        "    for image_id in sorted(task2_results.keys()):\n",
        "        writer.writerow([image_id, str(task2_results[image_id])])\n",
        "\n",
        "print(\"Task 2 prediction file saved successfully.\")\n",
        "print(\"Script finished.\")\n",
        "\n",
        "\n",
        "# --- (可選) 下載預測檔案 ---\n",
        "from google.colab import files\n",
        "files.download(TASK1_OUTPUT_PATH)\n",
        "files.download(TASK2_OUTPUT_PATH)\n",
        "files.download(MODEL_SAVE_PATH)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cuda\n",
            "Using torchvision v2 transforms: True\n",
            "Creating Datasets...\n",
            "loading annotations into memory...\n",
            "Done (t=0.38s)\n",
            "creating index...\n",
            "index created!\n",
            "Loaded 30062 images from data/your_dataset/train.json. Is test set: False\n",
            "loading annotations into memory...\n",
            "Done (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Loaded 3340 images from data/your_dataset/valid.json. Is test set: False\n",
            "loading annotations into memory...\n",
            "Done (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Loaded 13068 images from data/your_dataset/test.json. Is test set: True\n",
            "Creating DataLoaders...\n",
            "DataLoaders created.\n",
            "Loading model...\n",
            "Model loaded.\n",
            "Model weights loaded from fasterrcnn_svhn_best_model.pth\n",
            "Model ready.\n",
            "Setting up optimizer and scheduler...\n",
            "Optimizer and scheduler ready.\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "--- Epoch 1/10 --- Training ---\n",
            "Batch [100/3758], Current Avg Loss: 0.1620 (Last Batch Loss: 0.1679)\n",
            "Batch [200/3758], Current Avg Loss: 0.1655 (Last Batch Loss: 0.1846)\n",
            "Batch [300/3758], Current Avg Loss: 0.1646 (Last Batch Loss: 0.1416)\n",
            "Batch [400/3758], Current Avg Loss: 0.1631 (Last Batch Loss: 0.1283)\n",
            "Batch [500/3758], Current Avg Loss: 0.1620 (Last Batch Loss: 0.1277)\n",
            "Batch [600/3758], Current Avg Loss: 0.1628 (Last Batch Loss: 0.1340)\n",
            "Batch [700/3758], Current Avg Loss: 0.1628 (Last Batch Loss: 0.1814)\n",
            "Batch [800/3758], Current Avg Loss: 0.1625 (Last Batch Loss: 0.1817)\n",
            "Batch [900/3758], Current Avg Loss: 0.1623 (Last Batch Loss: 0.1613)\n",
            "Batch [1000/3758], Current Avg Loss: 0.1618 (Last Batch Loss: 0.1272)\n",
            "Batch [1100/3758], Current Avg Loss: 0.1619 (Last Batch Loss: 0.2104)\n",
            "Batch [1200/3758], Current Avg Loss: 0.1619 (Last Batch Loss: 0.1664)\n",
            "Batch [1300/3758], Current Avg Loss: 0.1618 (Last Batch Loss: 0.1926)\n",
            "Batch [1400/3758], Current Avg Loss: 0.1619 (Last Batch Loss: 0.1619)\n",
            "Batch [1500/3758], Current Avg Loss: 0.1621 (Last Batch Loss: 0.1621)\n",
            "Batch [1600/3758], Current Avg Loss: 0.1620 (Last Batch Loss: 0.2173)\n",
            "Batch [1700/3758], Current Avg Loss: 0.1623 (Last Batch Loss: 0.2554)\n",
            "Batch [1800/3758], Current Avg Loss: 0.1622 (Last Batch Loss: 0.1238)\n",
            "Batch [1900/3758], Current Avg Loss: 0.1622 (Last Batch Loss: 0.1265)\n",
            "Batch [2000/3758], Current Avg Loss: 0.1623 (Last Batch Loss: 0.1562)\n",
            "Batch [2100/3758], Current Avg Loss: 0.1624 (Last Batch Loss: 0.0919)\n",
            "Batch [2200/3758], Current Avg Loss: 0.1626 (Last Batch Loss: 0.1462)\n",
            "Batch [2300/3758], Current Avg Loss: 0.1623 (Last Batch Loss: 0.1305)\n",
            "Batch [2400/3758], Current Avg Loss: 0.1626 (Last Batch Loss: 0.1378)\n",
            "Batch [2500/3758], Current Avg Loss: 0.1627 (Last Batch Loss: 0.1621)\n",
            "Batch [2600/3758], Current Avg Loss: 0.1630 (Last Batch Loss: 0.1386)\n",
            "Batch [2700/3758], Current Avg Loss: 0.1628 (Last Batch Loss: 0.1715)\n",
            "Batch [2800/3758], Current Avg Loss: 0.1628 (Last Batch Loss: 0.1562)\n",
            "Batch [2900/3758], Current Avg Loss: 0.1627 (Last Batch Loss: 0.0785)\n",
            "Batch [3000/3758], Current Avg Loss: 0.1627 (Last Batch Loss: 0.1874)\n",
            "Batch [3100/3758], Current Avg Loss: 0.1629 (Last Batch Loss: 0.2142)\n",
            "Batch [3200/3758], Current Avg Loss: 0.1630 (Last Batch Loss: 0.1378)\n",
            "Batch [3300/3758], Current Avg Loss: 0.1630 (Last Batch Loss: 0.2000)\n",
            "Batch [3400/3758], Current Avg Loss: 0.1628 (Last Batch Loss: 0.1368)\n",
            "Batch [3500/3758], Current Avg Loss: 0.1628 (Last Batch Loss: 0.1358)\n",
            "Batch [3600/3758], Current Avg Loss: 0.1627 (Last Batch Loss: 0.1975)\n",
            "Batch [3700/3758], Current Avg Loss: 0.1626 (Last Batch Loss: 0.1029)\n",
            "Batch [3758/3758], Current Avg Loss: 0.1627 (Last Batch Loss: 0.2133)\n",
            "\n",
            "--- Epoch 1/10 --- Validation ---\n",
            "\n",
            "--- Epoch 1 Summary ---\n",
            "Average Training Loss: 0.1627\n",
            "Average Validation Loss: 0.1890\n",
            "Time Elapsed: 1443.19 seconds\n",
            "Current Learning Rate: 0.001000\n",
            "Validation loss improved to 0.1890. Model saved to fasterrcnn_svhn_best_model.pth\n",
            "------------------------------\n",
            "\n",
            "--- Epoch 2/10 --- Training ---\n",
            "Batch [100/3758], Current Avg Loss: 0.1582 (Last Batch Loss: 0.1606)\n",
            "Batch [200/3758], Current Avg Loss: 0.1568 (Last Batch Loss: 0.1033)\n",
            "Batch [300/3758], Current Avg Loss: 0.1559 (Last Batch Loss: 0.1690)\n",
            "Batch [400/3758], Current Avg Loss: 0.1547 (Last Batch Loss: 0.1385)\n",
            "Batch [500/3758], Current Avg Loss: 0.1564 (Last Batch Loss: 0.1761)\n",
            "Batch [600/3758], Current Avg Loss: 0.1571 (Last Batch Loss: 0.1450)\n",
            "Batch [700/3758], Current Avg Loss: 0.1567 (Last Batch Loss: 0.0778)\n",
            "Batch [800/3758], Current Avg Loss: 0.1566 (Last Batch Loss: 0.1525)\n",
            "Batch [900/3758], Current Avg Loss: 0.1565 (Last Batch Loss: 0.1164)\n",
            "Batch [1000/3758], Current Avg Loss: 0.1559 (Last Batch Loss: 0.1282)\n",
            "Batch [1100/3758], Current Avg Loss: 0.1561 (Last Batch Loss: 0.1838)\n",
            "Batch [1200/3758], Current Avg Loss: 0.1561 (Last Batch Loss: 0.1305)\n",
            "Batch [1300/3758], Current Avg Loss: 0.1562 (Last Batch Loss: 0.1970)\n",
            "Batch [1400/3758], Current Avg Loss: 0.1563 (Last Batch Loss: 0.1830)\n",
            "Batch [1500/3758], Current Avg Loss: 0.1563 (Last Batch Loss: 0.2077)\n",
            "Batch [1600/3758], Current Avg Loss: 0.1564 (Last Batch Loss: 0.2030)\n",
            "Batch [1700/3758], Current Avg Loss: 0.1566 (Last Batch Loss: 0.1458)\n",
            "Batch [1800/3758], Current Avg Loss: 0.1568 (Last Batch Loss: 0.2712)\n",
            "Batch [1900/3758], Current Avg Loss: 0.1567 (Last Batch Loss: 0.1406)\n",
            "Batch [2000/3758], Current Avg Loss: 0.1569 (Last Batch Loss: 0.1512)\n",
            "Batch [2100/3758], Current Avg Loss: 0.1569 (Last Batch Loss: 0.1229)\n",
            "Batch [2200/3758], Current Avg Loss: 0.1569 (Last Batch Loss: 0.1674)\n",
            "Batch [2300/3758], Current Avg Loss: 0.1570 (Last Batch Loss: 0.1082)\n",
            "Batch [2400/3758], Current Avg Loss: 0.1571 (Last Batch Loss: 0.1669)\n",
            "Batch [2500/3758], Current Avg Loss: 0.1570 (Last Batch Loss: 0.0927)\n",
            "Batch [2600/3758], Current Avg Loss: 0.1572 (Last Batch Loss: 0.2247)\n",
            "Batch [2700/3758], Current Avg Loss: 0.1572 (Last Batch Loss: 0.1643)\n",
            "Batch [2800/3758], Current Avg Loss: 0.1571 (Last Batch Loss: 0.2300)\n",
            "Batch [2900/3758], Current Avg Loss: 0.1571 (Last Batch Loss: 0.1442)\n",
            "Batch [3000/3758], Current Avg Loss: 0.1570 (Last Batch Loss: 0.1434)\n",
            "Batch [3100/3758], Current Avg Loss: 0.1569 (Last Batch Loss: 0.1292)\n",
            "Batch [3200/3758], Current Avg Loss: 0.1569 (Last Batch Loss: 0.1286)\n",
            "Batch [3300/3758], Current Avg Loss: 0.1570 (Last Batch Loss: 0.1207)\n",
            "Batch [3400/3758], Current Avg Loss: 0.1569 (Last Batch Loss: 0.1846)\n",
            "Batch [3500/3758], Current Avg Loss: 0.1569 (Last Batch Loss: 0.2207)\n",
            "Batch [3600/3758], Current Avg Loss: 0.1571 (Last Batch Loss: 0.1586)\n",
            "Batch [3700/3758], Current Avg Loss: 0.1570 (Last Batch Loss: 0.1272)\n",
            "Batch [3758/3758], Current Avg Loss: 0.1571 (Last Batch Loss: 0.1425)\n",
            "\n",
            "--- Epoch 2/10 --- Validation ---\n",
            "\n",
            "--- Epoch 2 Summary ---\n",
            "Average Training Loss: 0.1571\n",
            "Average Validation Loss: 0.1897\n",
            "Time Elapsed: 1442.83 seconds\n",
            "Current Learning Rate: 0.001000\n",
            "Validation loss (0.1897) did not improve from best (0.1890).\n",
            "------------------------------\n",
            "\n",
            "--- Epoch 3/10 --- Training ---\n",
            "Batch [100/3758], Current Avg Loss: 0.1498 (Last Batch Loss: 0.1475)\n",
            "Batch [200/3758], Current Avg Loss: 0.1514 (Last Batch Loss: 0.1127)\n",
            "Batch [300/3758], Current Avg Loss: 0.1513 (Last Batch Loss: 0.1248)\n",
            "Batch [400/3758], Current Avg Loss: 0.1513 (Last Batch Loss: 0.1922)\n",
            "Batch [500/3758], Current Avg Loss: 0.1512 (Last Batch Loss: 0.1443)\n",
            "Batch [600/3758], Current Avg Loss: 0.1505 (Last Batch Loss: 0.1253)\n",
            "Batch [700/3758], Current Avg Loss: 0.1513 (Last Batch Loss: 0.1482)\n",
            "Batch [800/3758], Current Avg Loss: 0.1509 (Last Batch Loss: 0.1732)\n",
            "Batch [900/3758], Current Avg Loss: 0.1513 (Last Batch Loss: 0.1609)\n",
            "Batch [1000/3758], Current Avg Loss: 0.1512 (Last Batch Loss: 0.2258)\n",
            "Batch [1100/3758], Current Avg Loss: 0.1514 (Last Batch Loss: 0.0797)\n",
            "Batch [1200/3758], Current Avg Loss: 0.1512 (Last Batch Loss: 0.1588)\n",
            "Batch [1300/3758], Current Avg Loss: 0.1509 (Last Batch Loss: 0.1369)\n",
            "Batch [1400/3758], Current Avg Loss: 0.1509 (Last Batch Loss: 0.2091)\n",
            "Batch [1500/3758], Current Avg Loss: 0.1505 (Last Batch Loss: 0.1848)\n",
            "Batch [1600/3758], Current Avg Loss: 0.1502 (Last Batch Loss: 0.1374)\n",
            "Batch [1700/3758], Current Avg Loss: 0.1504 (Last Batch Loss: 0.0882)\n",
            "Batch [1800/3758], Current Avg Loss: 0.1506 (Last Batch Loss: 0.1439)\n",
            "Batch [1900/3758], Current Avg Loss: 0.1507 (Last Batch Loss: 0.1447)\n",
            "Batch [2000/3758], Current Avg Loss: 0.1508 (Last Batch Loss: 0.1213)\n",
            "Batch [2100/3758], Current Avg Loss: 0.1510 (Last Batch Loss: 0.1464)\n",
            "Batch [2200/3758], Current Avg Loss: 0.1512 (Last Batch Loss: 0.1854)\n",
            "Batch [2300/3758], Current Avg Loss: 0.1511 (Last Batch Loss: 0.1787)\n",
            "Batch [2400/3758], Current Avg Loss: 0.1514 (Last Batch Loss: 0.1291)\n",
            "Batch [2500/3758], Current Avg Loss: 0.1516 (Last Batch Loss: 0.1359)\n",
            "Batch [2600/3758], Current Avg Loss: 0.1515 (Last Batch Loss: 0.1265)\n",
            "Batch [2700/3758], Current Avg Loss: 0.1515 (Last Batch Loss: 0.0890)\n",
            "Batch [2800/3758], Current Avg Loss: 0.1515 (Last Batch Loss: 0.1223)\n",
            "Batch [2900/3758], Current Avg Loss: 0.1515 (Last Batch Loss: 0.1404)\n",
            "Batch [3000/3758], Current Avg Loss: 0.1515 (Last Batch Loss: 0.1532)\n",
            "Batch [3100/3758], Current Avg Loss: 0.1517 (Last Batch Loss: 0.2507)\n",
            "Batch [3200/3758], Current Avg Loss: 0.1517 (Last Batch Loss: 0.1327)\n",
            "Batch [3300/3758], Current Avg Loss: 0.1518 (Last Batch Loss: 0.1504)\n",
            "Batch [3400/3758], Current Avg Loss: 0.1518 (Last Batch Loss: 0.1338)\n",
            "Batch [3500/3758], Current Avg Loss: 0.1516 (Last Batch Loss: 0.1102)\n",
            "Batch [3600/3758], Current Avg Loss: 0.1516 (Last Batch Loss: 0.2242)\n",
            "Batch [3700/3758], Current Avg Loss: 0.1516 (Last Batch Loss: 0.1457)\n",
            "Batch [3758/3758], Current Avg Loss: 0.1517 (Last Batch Loss: 0.1553)\n",
            "\n",
            "--- Epoch 3/10 --- Validation ---\n",
            "\n",
            "--- Epoch 3 Summary ---\n",
            "Average Training Loss: 0.1517\n",
            "Average Validation Loss: 0.1900\n",
            "Time Elapsed: 1443.00 seconds\n",
            "Current Learning Rate: 0.001000\n",
            "Validation loss (0.1900) did not improve from best (0.1890).\n",
            "------------------------------\n",
            "\n",
            "--- Epoch 4/10 --- Training ---\n",
            "Batch [100/3758], Current Avg Loss: 0.1458 (Last Batch Loss: 0.1375)\n",
            "Batch [200/3758], Current Avg Loss: 0.1428 (Last Batch Loss: 0.1587)\n",
            "Batch [300/3758], Current Avg Loss: 0.1440 (Last Batch Loss: 0.1414)\n",
            "Batch [400/3758], Current Avg Loss: 0.1437 (Last Batch Loss: 0.1304)\n",
            "Batch [500/3758], Current Avg Loss: 0.1436 (Last Batch Loss: 0.1289)\n",
            "Batch [600/3758], Current Avg Loss: 0.1427 (Last Batch Loss: 0.1025)\n",
            "Batch [700/3758], Current Avg Loss: 0.1433 (Last Batch Loss: 0.1374)\n",
            "Batch [800/3758], Current Avg Loss: 0.1439 (Last Batch Loss: 0.1667)\n",
            "Batch [900/3758], Current Avg Loss: 0.1435 (Last Batch Loss: 0.1823)\n",
            "Batch [1000/3758], Current Avg Loss: 0.1440 (Last Batch Loss: 0.1505)\n",
            "Batch [1100/3758], Current Avg Loss: 0.1447 (Last Batch Loss: 0.1667)\n",
            "Batch [1200/3758], Current Avg Loss: 0.1446 (Last Batch Loss: 0.1667)\n",
            "Batch [1300/3758], Current Avg Loss: 0.1448 (Last Batch Loss: 0.1276)\n",
            "Batch [1400/3758], Current Avg Loss: 0.1442 (Last Batch Loss: 0.1213)\n",
            "Batch [1500/3758], Current Avg Loss: 0.1443 (Last Batch Loss: 0.1046)\n",
            "Batch [1600/3758], Current Avg Loss: 0.1445 (Last Batch Loss: 0.1323)\n",
            "Batch [1700/3758], Current Avg Loss: 0.1446 (Last Batch Loss: 0.1336)\n",
            "Batch [1800/3758], Current Avg Loss: 0.1446 (Last Batch Loss: 0.1551)\n",
            "Batch [1900/3758], Current Avg Loss: 0.1446 (Last Batch Loss: 0.1480)\n",
            "Batch [2000/3758], Current Avg Loss: 0.1447 (Last Batch Loss: 0.2401)\n",
            "Batch [2100/3758], Current Avg Loss: 0.1449 (Last Batch Loss: 0.1403)\n",
            "Batch [2200/3758], Current Avg Loss: 0.1449 (Last Batch Loss: 0.1292)\n",
            "Batch [2300/3758], Current Avg Loss: 0.1452 (Last Batch Loss: 0.1646)\n",
            "Batch [2400/3758], Current Avg Loss: 0.1453 (Last Batch Loss: 0.1088)\n",
            "Batch [2500/3758], Current Avg Loss: 0.1454 (Last Batch Loss: 0.1381)\n",
            "Batch [2600/3758], Current Avg Loss: 0.1455 (Last Batch Loss: 0.1318)\n",
            "Batch [2700/3758], Current Avg Loss: 0.1454 (Last Batch Loss: 0.2833)\n",
            "Batch [2800/3758], Current Avg Loss: 0.1454 (Last Batch Loss: 0.1291)\n",
            "Batch [2900/3758], Current Avg Loss: 0.1456 (Last Batch Loss: 0.1905)\n",
            "Batch [3000/3758], Current Avg Loss: 0.1457 (Last Batch Loss: 0.1736)\n",
            "Batch [3100/3758], Current Avg Loss: 0.1458 (Last Batch Loss: 0.1627)\n",
            "Batch [3200/3758], Current Avg Loss: 0.1459 (Last Batch Loss: 0.1526)\n",
            "Batch [3300/3758], Current Avg Loss: 0.1459 (Last Batch Loss: 0.1750)\n",
            "Batch [3400/3758], Current Avg Loss: 0.1458 (Last Batch Loss: 0.1433)\n",
            "Batch [3500/3758], Current Avg Loss: 0.1458 (Last Batch Loss: 0.1601)\n",
            "Batch [3600/3758], Current Avg Loss: 0.1457 (Last Batch Loss: 0.1309)\n",
            "Batch [3700/3758], Current Avg Loss: 0.1458 (Last Batch Loss: 0.2209)\n",
            "Batch [3758/3758], Current Avg Loss: 0.1458 (Last Batch Loss: 0.1223)\n",
            "\n",
            "--- Epoch 4/10 --- Validation ---\n",
            "\n",
            "--- Epoch 4 Summary ---\n",
            "Average Training Loss: 0.1458\n",
            "Average Validation Loss: 0.1940\n",
            "Time Elapsed: 1443.39 seconds\n",
            "Current Learning Rate: 0.001000\n",
            "Validation loss (0.1940) did not improve from best (0.1890).\n",
            "------------------------------\n",
            "\n",
            "--- Epoch 5/10 --- Training ---\n",
            "Batch [100/3758], Current Avg Loss: 0.1373 (Last Batch Loss: 0.1870)\n",
            "Batch [200/3758], Current Avg Loss: 0.1386 (Last Batch Loss: 0.1790)\n",
            "Batch [300/3758], Current Avg Loss: 0.1355 (Last Batch Loss: 0.1347)\n",
            "Batch [400/3758], Current Avg Loss: 0.1348 (Last Batch Loss: 0.1347)\n",
            "Batch [500/3758], Current Avg Loss: 0.1341 (Last Batch Loss: 0.2629)\n",
            "Batch [600/3758], Current Avg Loss: 0.1340 (Last Batch Loss: 0.1488)\n",
            "Batch [700/3758], Current Avg Loss: 0.1339 (Last Batch Loss: 0.1546)\n",
            "Batch [800/3758], Current Avg Loss: 0.1338 (Last Batch Loss: 0.1839)\n",
            "Batch [900/3758], Current Avg Loss: 0.1336 (Last Batch Loss: 0.1753)\n",
            "Batch [1000/3758], Current Avg Loss: 0.1337 (Last Batch Loss: 0.1247)\n",
            "Batch [1100/3758], Current Avg Loss: 0.1334 (Last Batch Loss: 0.1624)\n",
            "Batch [1200/3758], Current Avg Loss: 0.1335 (Last Batch Loss: 0.1113)\n",
            "Batch [1300/3758], Current Avg Loss: 0.1332 (Last Batch Loss: 0.1151)\n",
            "Batch [1400/3758], Current Avg Loss: 0.1334 (Last Batch Loss: 0.1503)\n",
            "Batch [1500/3758], Current Avg Loss: 0.1333 (Last Batch Loss: 0.1289)\n",
            "Batch [1600/3758], Current Avg Loss: 0.1332 (Last Batch Loss: 0.1697)\n",
            "Batch [1700/3758], Current Avg Loss: 0.1331 (Last Batch Loss: 0.1130)\n",
            "Batch [1800/3758], Current Avg Loss: 0.1327 (Last Batch Loss: 0.1394)\n",
            "Batch [1900/3758], Current Avg Loss: 0.1328 (Last Batch Loss: 0.1621)\n",
            "Batch [2000/3758], Current Avg Loss: 0.1326 (Last Batch Loss: 0.0989)\n",
            "Batch [2100/3758], Current Avg Loss: 0.1325 (Last Batch Loss: 0.1467)\n",
            "Batch [2200/3758], Current Avg Loss: 0.1325 (Last Batch Loss: 0.1573)\n",
            "Batch [2300/3758], Current Avg Loss: 0.1325 (Last Batch Loss: 0.1116)\n",
            "Batch [2400/3758], Current Avg Loss: 0.1324 (Last Batch Loss: 0.1429)\n",
            "Batch [2500/3758], Current Avg Loss: 0.1323 (Last Batch Loss: 0.1323)\n",
            "Batch [2600/3758], Current Avg Loss: 0.1323 (Last Batch Loss: 0.1522)\n",
            "Batch [2700/3758], Current Avg Loss: 0.1322 (Last Batch Loss: 0.0657)\n",
            "Batch [2800/3758], Current Avg Loss: 0.1321 (Last Batch Loss: 0.1414)\n",
            "Batch [2900/3758], Current Avg Loss: 0.1320 (Last Batch Loss: 0.1286)\n",
            "Batch [3000/3758], Current Avg Loss: 0.1318 (Last Batch Loss: 0.1402)\n",
            "Batch [3100/3758], Current Avg Loss: 0.1317 (Last Batch Loss: 0.1261)\n",
            "Batch [3200/3758], Current Avg Loss: 0.1318 (Last Batch Loss: 0.1275)\n",
            "Batch [3300/3758], Current Avg Loss: 0.1318 (Last Batch Loss: 0.1399)\n",
            "Batch [3400/3758], Current Avg Loss: 0.1317 (Last Batch Loss: 0.1312)\n",
            "Batch [3500/3758], Current Avg Loss: 0.1317 (Last Batch Loss: 0.1026)\n",
            "Batch [3600/3758], Current Avg Loss: 0.1317 (Last Batch Loss: 0.0948)\n",
            "Batch [3700/3758], Current Avg Loss: 0.1317 (Last Batch Loss: 0.1235)\n",
            "Batch [3758/3758], Current Avg Loss: 0.1317 (Last Batch Loss: 0.1734)\n",
            "\n",
            "--- Epoch 5/10 --- Validation ---\n",
            "\n",
            "--- Epoch 5 Summary ---\n",
            "Average Training Loss: 0.1317\n",
            "Average Validation Loss: 0.1981\n",
            "Time Elapsed: 1444.18 seconds\n",
            "Current Learning Rate: 0.000100\n",
            "Validation loss (0.1981) did not improve from best (0.1890).\n",
            "------------------------------\n",
            "\n",
            "--- Epoch 6/10 --- Training ---\n",
            "Batch [100/3758], Current Avg Loss: 0.1289 (Last Batch Loss: 0.1296)\n",
            "Batch [200/3758], Current Avg Loss: 0.1274 (Last Batch Loss: 0.1276)\n",
            "Batch [300/3758], Current Avg Loss: 0.1273 (Last Batch Loss: 0.1034)\n",
            "Batch [400/3758], Current Avg Loss: 0.1276 (Last Batch Loss: 0.1152)\n",
            "Batch [500/3758], Current Avg Loss: 0.1274 (Last Batch Loss: 0.1037)\n",
            "Batch [600/3758], Current Avg Loss: 0.1266 (Last Batch Loss: 0.1307)\n",
            "Batch [700/3758], Current Avg Loss: 0.1272 (Last Batch Loss: 0.1849)\n",
            "Batch [800/3758], Current Avg Loss: 0.1267 (Last Batch Loss: 0.1506)\n",
            "Batch [900/3758], Current Avg Loss: 0.1264 (Last Batch Loss: 0.1537)\n",
            "Batch [1000/3758], Current Avg Loss: 0.1264 (Last Batch Loss: 0.1300)\n",
            "Batch [1100/3758], Current Avg Loss: 0.1260 (Last Batch Loss: 0.1449)\n",
            "Batch [1200/3758], Current Avg Loss: 0.1258 (Last Batch Loss: 0.0913)\n",
            "Batch [1300/3758], Current Avg Loss: 0.1257 (Last Batch Loss: 0.1444)\n",
            "Batch [1400/3758], Current Avg Loss: 0.1257 (Last Batch Loss: 0.1381)\n",
            "Batch [1500/3758], Current Avg Loss: 0.1257 (Last Batch Loss: 0.1144)\n",
            "Batch [1600/3758], Current Avg Loss: 0.1259 (Last Batch Loss: 0.0889)\n",
            "Batch [1700/3758], Current Avg Loss: 0.1259 (Last Batch Loss: 0.1003)\n",
            "Batch [1800/3758], Current Avg Loss: 0.1257 (Last Batch Loss: 0.1023)\n",
            "Batch [1900/3758], Current Avg Loss: 0.1257 (Last Batch Loss: 0.1043)\n",
            "Batch [2000/3758], Current Avg Loss: 0.1258 (Last Batch Loss: 0.1364)\n",
            "Batch [2100/3758], Current Avg Loss: 0.1258 (Last Batch Loss: 0.1337)\n",
            "Batch [2200/3758], Current Avg Loss: 0.1258 (Last Batch Loss: 0.1521)\n",
            "Batch [2300/3758], Current Avg Loss: 0.1258 (Last Batch Loss: 0.1184)\n",
            "Batch [2400/3758], Current Avg Loss: 0.1260 (Last Batch Loss: 0.1145)\n",
            "Batch [2500/3758], Current Avg Loss: 0.1260 (Last Batch Loss: 0.1238)\n",
            "Batch [2600/3758], Current Avg Loss: 0.1261 (Last Batch Loss: 0.1294)\n",
            "Batch [2700/3758], Current Avg Loss: 0.1260 (Last Batch Loss: 0.1267)\n",
            "Batch [2800/3758], Current Avg Loss: 0.1260 (Last Batch Loss: 0.0892)\n",
            "Batch [2900/3758], Current Avg Loss: 0.1260 (Last Batch Loss: 0.0911)\n",
            "Batch [3000/3758], Current Avg Loss: 0.1261 (Last Batch Loss: 0.1339)\n",
            "Batch [3100/3758], Current Avg Loss: 0.1260 (Last Batch Loss: 0.1731)\n",
            "Batch [3200/3758], Current Avg Loss: 0.1258 (Last Batch Loss: 0.0938)\n",
            "Batch [3300/3758], Current Avg Loss: 0.1259 (Last Batch Loss: 0.1413)\n",
            "Batch [3400/3758], Current Avg Loss: 0.1259 (Last Batch Loss: 0.1240)\n",
            "Batch [3500/3758], Current Avg Loss: 0.1260 (Last Batch Loss: 0.1439)\n",
            "Batch [3600/3758], Current Avg Loss: 0.1259 (Last Batch Loss: 0.1173)\n",
            "Batch [3700/3758], Current Avg Loss: 0.1259 (Last Batch Loss: 0.1196)\n",
            "Batch [3758/3758], Current Avg Loss: 0.1260 (Last Batch Loss: 0.1644)\n",
            "\n",
            "--- Epoch 6/10 --- Validation ---\n",
            "\n",
            "--- Epoch 6 Summary ---\n",
            "Average Training Loss: 0.1260\n",
            "Average Validation Loss: 0.2004\n",
            "Time Elapsed: 1443.83 seconds\n",
            "Current Learning Rate: 0.000100\n",
            "Validation loss (0.2004) did not improve from best (0.1890).\n",
            "------------------------------\n",
            "\n",
            "--- Epoch 7/10 --- Training ---\n",
            "Batch [100/3758], Current Avg Loss: 0.1204 (Last Batch Loss: 0.0932)\n",
            "Batch [200/3758], Current Avg Loss: 0.1207 (Last Batch Loss: 0.0963)\n",
            "Batch [300/3758], Current Avg Loss: 0.1208 (Last Batch Loss: 0.1078)\n",
            "Batch [400/3758], Current Avg Loss: 0.1199 (Last Batch Loss: 0.1042)\n",
            "Batch [500/3758], Current Avg Loss: 0.1206 (Last Batch Loss: 0.0945)\n",
            "Batch [600/3758], Current Avg Loss: 0.1209 (Last Batch Loss: 0.0980)\n",
            "Batch [700/3758], Current Avg Loss: 0.1208 (Last Batch Loss: 0.1137)\n",
            "Batch [800/3758], Current Avg Loss: 0.1215 (Last Batch Loss: 0.1744)\n",
            "Batch [900/3758], Current Avg Loss: 0.1219 (Last Batch Loss: 0.1580)\n",
            "Batch [1000/3758], Current Avg Loss: 0.1220 (Last Batch Loss: 0.1068)\n",
            "Batch [1100/3758], Current Avg Loss: 0.1218 (Last Batch Loss: 0.1667)\n",
            "Batch [1200/3758], Current Avg Loss: 0.1216 (Last Batch Loss: 0.0972)\n",
            "Batch [1300/3758], Current Avg Loss: 0.1218 (Last Batch Loss: 0.1060)\n",
            "Batch [1400/3758], Current Avg Loss: 0.1216 (Last Batch Loss: 0.1018)\n",
            "Batch [1500/3758], Current Avg Loss: 0.1216 (Last Batch Loss: 0.1316)\n",
            "Batch [1600/3758], Current Avg Loss: 0.1213 (Last Batch Loss: 0.1174)\n",
            "Batch [1700/3758], Current Avg Loss: 0.1213 (Last Batch Loss: 0.1106)\n",
            "Batch [1800/3758], Current Avg Loss: 0.1215 (Last Batch Loss: 0.1370)\n",
            "Batch [1900/3758], Current Avg Loss: 0.1214 (Last Batch Loss: 0.1237)\n",
            "Batch [2000/3758], Current Avg Loss: 0.1216 (Last Batch Loss: 0.1551)\n",
            "Batch [2100/3758], Current Avg Loss: 0.1217 (Last Batch Loss: 0.0911)\n",
            "Batch [2200/3758], Current Avg Loss: 0.1217 (Last Batch Loss: 0.0949)\n",
            "Batch [2300/3758], Current Avg Loss: 0.1218 (Last Batch Loss: 0.1458)\n",
            "Batch [2400/3758], Current Avg Loss: 0.1218 (Last Batch Loss: 0.1398)\n",
            "Batch [2500/3758], Current Avg Loss: 0.1219 (Last Batch Loss: 0.1242)\n",
            "Batch [2600/3758], Current Avg Loss: 0.1218 (Last Batch Loss: 0.0976)\n",
            "Batch [2700/3758], Current Avg Loss: 0.1216 (Last Batch Loss: 0.1016)\n",
            "Batch [2800/3758], Current Avg Loss: 0.1216 (Last Batch Loss: 0.0989)\n",
            "Batch [2900/3758], Current Avg Loss: 0.1217 (Last Batch Loss: 0.0993)\n",
            "Batch [3000/3758], Current Avg Loss: 0.1217 (Last Batch Loss: 0.1238)\n",
            "Batch [3100/3758], Current Avg Loss: 0.1217 (Last Batch Loss: 0.1356)\n",
            "Batch [3200/3758], Current Avg Loss: 0.1219 (Last Batch Loss: 0.1438)\n",
            "Batch [3300/3758], Current Avg Loss: 0.1221 (Last Batch Loss: 0.1219)\n",
            "Batch [3400/3758], Current Avg Loss: 0.1222 (Last Batch Loss: 0.1047)\n",
            "Batch [3500/3758], Current Avg Loss: 0.1221 (Last Batch Loss: 0.0988)\n",
            "Batch [3600/3758], Current Avg Loss: 0.1221 (Last Batch Loss: 0.1522)\n",
            "Batch [3700/3758], Current Avg Loss: 0.1221 (Last Batch Loss: 0.1627)\n",
            "Batch [3758/3758], Current Avg Loss: 0.1220 (Last Batch Loss: 0.1065)\n",
            "\n",
            "--- Epoch 7/10 --- Validation ---\n",
            "\n",
            "--- Epoch 7 Summary ---\n",
            "Average Training Loss: 0.1220\n",
            "Average Validation Loss: 0.2055\n",
            "Time Elapsed: 1444.96 seconds\n",
            "Current Learning Rate: 0.000100\n",
            "Validation loss (0.2055) did not improve from best (0.1890).\n",
            "------------------------------\n",
            "\n",
            "--- Epoch 8/10 --- Training ---\n",
            "Batch [100/3758], Current Avg Loss: 0.1192 (Last Batch Loss: 0.1014)\n",
            "Batch [200/3758], Current Avg Loss: 0.1171 (Last Batch Loss: 0.0940)\n",
            "Batch [300/3758], Current Avg Loss: 0.1166 (Last Batch Loss: 0.1497)\n",
            "Batch [400/3758], Current Avg Loss: 0.1170 (Last Batch Loss: 0.1214)\n",
            "Batch [500/3758], Current Avg Loss: 0.1166 (Last Batch Loss: 0.1287)\n",
            "Batch [600/3758], Current Avg Loss: 0.1163 (Last Batch Loss: 0.1250)\n",
            "Batch [700/3758], Current Avg Loss: 0.1166 (Last Batch Loss: 0.1186)\n",
            "Batch [800/3758], Current Avg Loss: 0.1166 (Last Batch Loss: 0.1407)\n",
            "Batch [900/3758], Current Avg Loss: 0.1171 (Last Batch Loss: 0.1248)\n",
            "Batch [1000/3758], Current Avg Loss: 0.1169 (Last Batch Loss: 0.1517)\n",
            "Batch [1100/3758], Current Avg Loss: 0.1170 (Last Batch Loss: 0.1085)\n",
            "Batch [1200/3758], Current Avg Loss: 0.1172 (Last Batch Loss: 0.1091)\n",
            "Batch [1300/3758], Current Avg Loss: 0.1172 (Last Batch Loss: 0.0850)\n",
            "Batch [1400/3758], Current Avg Loss: 0.1170 (Last Batch Loss: 0.0876)\n",
            "Batch [1500/3758], Current Avg Loss: 0.1168 (Last Batch Loss: 0.0815)\n",
            "Batch [1600/3758], Current Avg Loss: 0.1168 (Last Batch Loss: 0.1080)\n",
            "Batch [1700/3758], Current Avg Loss: 0.1168 (Last Batch Loss: 0.1108)\n",
            "Batch [1800/3758], Current Avg Loss: 0.1171 (Last Batch Loss: 0.0872)\n",
            "Batch [1900/3758], Current Avg Loss: 0.1170 (Last Batch Loss: 0.1046)\n",
            "Batch [2000/3758], Current Avg Loss: 0.1168 (Last Batch Loss: 0.1223)\n",
            "Batch [2100/3758], Current Avg Loss: 0.1169 (Last Batch Loss: 0.1216)\n",
            "Batch [2200/3758], Current Avg Loss: 0.1169 (Last Batch Loss: 0.1140)\n",
            "Batch [2300/3758], Current Avg Loss: 0.1171 (Last Batch Loss: 0.1262)\n",
            "Batch [2400/3758], Current Avg Loss: 0.1170 (Last Batch Loss: 0.0869)\n",
            "Batch [2500/3758], Current Avg Loss: 0.1171 (Last Batch Loss: 0.1861)\n",
            "Batch [2600/3758], Current Avg Loss: 0.1171 (Last Batch Loss: 0.1244)\n",
            "Batch [2700/3758], Current Avg Loss: 0.1171 (Last Batch Loss: 0.0963)\n",
            "Batch [2800/3758], Current Avg Loss: 0.1171 (Last Batch Loss: 0.1014)\n",
            "Batch [2900/3758], Current Avg Loss: 0.1171 (Last Batch Loss: 0.1111)\n",
            "Batch [3000/3758], Current Avg Loss: 0.1171 (Last Batch Loss: 0.0987)\n",
            "Batch [3100/3758], Current Avg Loss: 0.1171 (Last Batch Loss: 0.1172)\n",
            "Batch [3200/3758], Current Avg Loss: 0.1171 (Last Batch Loss: 0.0960)\n",
            "Batch [3300/3758], Current Avg Loss: 0.1171 (Last Batch Loss: 0.1652)\n",
            "Batch [3400/3758], Current Avg Loss: 0.1171 (Last Batch Loss: 0.1642)\n",
            "Batch [3500/3758], Current Avg Loss: 0.1172 (Last Batch Loss: 0.1858)\n",
            "Batch [3600/3758], Current Avg Loss: 0.1171 (Last Batch Loss: 0.0867)\n",
            "Batch [3700/3758], Current Avg Loss: 0.1171 (Last Batch Loss: 0.0753)\n",
            "Batch [3758/3758], Current Avg Loss: 0.1171 (Last Batch Loss: 0.0938)\n",
            "\n",
            "--- Epoch 8/10 --- Validation ---\n",
            "\n",
            "--- Epoch 8 Summary ---\n",
            "Average Training Loss: 0.1171\n",
            "Average Validation Loss: 0.2067\n",
            "Time Elapsed: 1449.44 seconds\n",
            "Current Learning Rate: 0.000010\n",
            "Validation loss (0.2067) did not improve from best (0.1890).\n",
            "------------------------------\n",
            "\n",
            "--- Epoch 9/10 --- Training ---\n",
            "Batch [100/3758], Current Avg Loss: 0.1153 (Last Batch Loss: 0.1352)\n",
            "Batch [200/3758], Current Avg Loss: 0.1145 (Last Batch Loss: 0.1003)\n",
            "Batch [300/3758], Current Avg Loss: 0.1142 (Last Batch Loss: 0.1702)\n",
            "Batch [400/3758], Current Avg Loss: 0.1138 (Last Batch Loss: 0.0942)\n",
            "Batch [500/3758], Current Avg Loss: 0.1137 (Last Batch Loss: 0.1102)\n",
            "Batch [600/3758], Current Avg Loss: 0.1138 (Last Batch Loss: 0.0909)\n",
            "Batch [700/3758], Current Avg Loss: 0.1149 (Last Batch Loss: 0.1174)\n",
            "Batch [800/3758], Current Avg Loss: 0.1153 (Last Batch Loss: 0.1399)\n",
            "Batch [900/3758], Current Avg Loss: 0.1158 (Last Batch Loss: 0.1215)\n",
            "Batch [1000/3758], Current Avg Loss: 0.1156 (Last Batch Loss: 0.1350)\n",
            "Batch [1100/3758], Current Avg Loss: 0.1158 (Last Batch Loss: 0.0964)\n",
            "Batch [1200/3758], Current Avg Loss: 0.1158 (Last Batch Loss: 0.1298)\n",
            "Batch [1300/3758], Current Avg Loss: 0.1158 (Last Batch Loss: 0.1534)\n",
            "Batch [1400/3758], Current Avg Loss: 0.1158 (Last Batch Loss: 0.1073)\n",
            "Batch [1500/3758], Current Avg Loss: 0.1159 (Last Batch Loss: 0.1043)\n",
            "Batch [1600/3758], Current Avg Loss: 0.1160 (Last Batch Loss: 0.1534)\n",
            "Batch [1700/3758], Current Avg Loss: 0.1161 (Last Batch Loss: 0.1277)\n",
            "Batch [1800/3758], Current Avg Loss: 0.1163 (Last Batch Loss: 0.1407)\n",
            "Batch [1900/3758], Current Avg Loss: 0.1161 (Last Batch Loss: 0.1178)\n",
            "Batch [2000/3758], Current Avg Loss: 0.1162 (Last Batch Loss: 0.1478)\n",
            "Batch [2100/3758], Current Avg Loss: 0.1163 (Last Batch Loss: 0.1452)\n",
            "Batch [2200/3758], Current Avg Loss: 0.1164 (Last Batch Loss: 0.0862)\n",
            "Batch [2300/3758], Current Avg Loss: 0.1165 (Last Batch Loss: 0.0985)\n",
            "Batch [2400/3758], Current Avg Loss: 0.1164 (Last Batch Loss: 0.1266)\n",
            "Batch [2500/3758], Current Avg Loss: 0.1165 (Last Batch Loss: 0.0966)\n",
            "Batch [2600/3758], Current Avg Loss: 0.1166 (Last Batch Loss: 0.1298)\n",
            "Batch [2700/3758], Current Avg Loss: 0.1165 (Last Batch Loss: 0.1114)\n",
            "Batch [2800/3758], Current Avg Loss: 0.1165 (Last Batch Loss: 0.1342)\n",
            "Batch [2900/3758], Current Avg Loss: 0.1167 (Last Batch Loss: 0.1048)\n",
            "Batch [3000/3758], Current Avg Loss: 0.1165 (Last Batch Loss: 0.0775)\n",
            "Batch [3100/3758], Current Avg Loss: 0.1166 (Last Batch Loss: 0.0992)\n",
            "Batch [3200/3758], Current Avg Loss: 0.1166 (Last Batch Loss: 0.0964)\n",
            "Batch [3300/3758], Current Avg Loss: 0.1165 (Last Batch Loss: 0.1281)\n",
            "Batch [3400/3758], Current Avg Loss: 0.1165 (Last Batch Loss: 0.1021)\n",
            "Batch [3500/3758], Current Avg Loss: 0.1165 (Last Batch Loss: 0.1136)\n",
            "Batch [3600/3758], Current Avg Loss: 0.1165 (Last Batch Loss: 0.1206)\n",
            "Batch [3700/3758], Current Avg Loss: 0.1165 (Last Batch Loss: 0.0798)\n",
            "Batch [3758/3758], Current Avg Loss: 0.1164 (Last Batch Loss: 0.1070)\n",
            "\n",
            "--- Epoch 9/10 --- Validation ---\n",
            "\n",
            "--- Epoch 9 Summary ---\n",
            "Average Training Loss: 0.1164\n",
            "Average Validation Loss: 0.2069\n",
            "Time Elapsed: 1446.71 seconds\n",
            "Current Learning Rate: 0.000010\n",
            "Validation loss (0.2069) did not improve from best (0.1890).\n",
            "------------------------------\n",
            "\n",
            "--- Epoch 10/10 --- Training ---\n",
            "Batch [100/3758], Current Avg Loss: 0.1155 (Last Batch Loss: 0.0855)\n",
            "Batch [200/3758], Current Avg Loss: 0.1154 (Last Batch Loss: 0.1001)\n",
            "Batch [300/3758], Current Avg Loss: 0.1157 (Last Batch Loss: 0.1027)\n",
            "Batch [400/3758], Current Avg Loss: 0.1158 (Last Batch Loss: 0.1135)\n",
            "Batch [500/3758], Current Avg Loss: 0.1150 (Last Batch Loss: 0.1029)\n",
            "Batch [600/3758], Current Avg Loss: 0.1153 (Last Batch Loss: 0.1354)\n",
            "Batch [700/3758], Current Avg Loss: 0.1152 (Last Batch Loss: 0.0916)\n",
            "Batch [800/3758], Current Avg Loss: 0.1158 (Last Batch Loss: 0.1117)\n",
            "Batch [900/3758], Current Avg Loss: 0.1159 (Last Batch Loss: 0.1225)\n",
            "Batch [1000/3758], Current Avg Loss: 0.1159 (Last Batch Loss: 0.1230)\n",
            "Batch [1100/3758], Current Avg Loss: 0.1157 (Last Batch Loss: 0.1030)\n",
            "Batch [1200/3758], Current Avg Loss: 0.1157 (Last Batch Loss: 0.0912)\n",
            "Batch [1300/3758], Current Avg Loss: 0.1157 (Last Batch Loss: 0.0949)\n",
            "Batch [1400/3758], Current Avg Loss: 0.1155 (Last Batch Loss: 0.1088)\n",
            "Batch [1500/3758], Current Avg Loss: 0.1156 (Last Batch Loss: 0.1058)\n",
            "Batch [1600/3758], Current Avg Loss: 0.1159 (Last Batch Loss: 0.0974)\n",
            "Batch [1700/3758], Current Avg Loss: 0.1161 (Last Batch Loss: 0.1235)\n",
            "Batch [1800/3758], Current Avg Loss: 0.1160 (Last Batch Loss: 0.1353)\n",
            "Batch [1900/3758], Current Avg Loss: 0.1163 (Last Batch Loss: 0.1590)\n",
            "Batch [2000/3758], Current Avg Loss: 0.1163 (Last Batch Loss: 0.1152)\n",
            "Batch [2100/3758], Current Avg Loss: 0.1162 (Last Batch Loss: 0.1124)\n",
            "Batch [2200/3758], Current Avg Loss: 0.1164 (Last Batch Loss: 0.1103)\n",
            "Batch [2300/3758], Current Avg Loss: 0.1163 (Last Batch Loss: 0.1194)\n",
            "Batch [2400/3758], Current Avg Loss: 0.1162 (Last Batch Loss: 0.1108)\n",
            "Batch [2500/3758], Current Avg Loss: 0.1160 (Last Batch Loss: 0.0989)\n",
            "Batch [2600/3758], Current Avg Loss: 0.1160 (Last Batch Loss: 0.1256)\n",
            "Batch [2700/3758], Current Avg Loss: 0.1158 (Last Batch Loss: 0.1123)\n",
            "Batch [2800/3758], Current Avg Loss: 0.1159 (Last Batch Loss: 0.1180)\n",
            "Batch [2900/3758], Current Avg Loss: 0.1158 (Last Batch Loss: 0.1715)\n",
            "Batch [3000/3758], Current Avg Loss: 0.1156 (Last Batch Loss: 0.1764)\n",
            "Batch [3100/3758], Current Avg Loss: 0.1155 (Last Batch Loss: 0.1005)\n",
            "Batch [3200/3758], Current Avg Loss: 0.1155 (Last Batch Loss: 0.0862)\n",
            "Batch [3300/3758], Current Avg Loss: 0.1155 (Last Batch Loss: 0.1142)\n",
            "Batch [3400/3758], Current Avg Loss: 0.1154 (Last Batch Loss: 0.1329)\n",
            "Batch [3500/3758], Current Avg Loss: 0.1154 (Last Batch Loss: 0.1000)\n",
            "Batch [3600/3758], Current Avg Loss: 0.1155 (Last Batch Loss: 0.1308)\n",
            "Batch [3700/3758], Current Avg Loss: 0.1155 (Last Batch Loss: 0.1252)\n",
            "Batch [3758/3758], Current Avg Loss: 0.1155 (Last Batch Loss: 0.1161)\n",
            "\n",
            "--- Epoch 10/10 --- Validation ---\n",
            "\n",
            "--- Epoch 10 Summary ---\n",
            "Average Training Loss: 0.1155\n",
            "Average Validation Loss: 0.2076\n",
            "Time Elapsed: 1445.28 seconds\n",
            "Current Learning Rate: 0.000010\n",
            "Validation loss (0.2076) did not improve from best (0.1890).\n",
            "------------------------------\n",
            "\n",
            "--- Training Finished ---\n",
            "Total Training Time: 14447.24 seconds (240.79 minutes)\n",
            "Best Validation Loss Achieved: 0.1890\n",
            "\n",
            "--- Loading best model for final prediction ---\n",
            "Loaded best model from fasterrcnn_svhn_best_model.pth with validation loss 0.1890\n",
            "\n",
            "--- Generating Task 1 predictions (pred.json) ---\n",
            "Task 1: Processing test image 1000/13068\n",
            "Task 1: Processing test image 2000/13068\n",
            "Task 1: Processing test image 3000/13068\n",
            "Task 1: Processing test image 4000/13068\n",
            "Task 1: Processing test image 5000/13068\n",
            "Task 1: Processing test image 6000/13068\n",
            "Task 1: Processing test image 7000/13068\n",
            "Task 1: Processing test image 8000/13068\n",
            "Task 1: Processing test image 9000/13068\n",
            "Task 1: Processing test image 10000/13068\n",
            "Task 1: Processing test image 11000/13068\n",
            "Task 1: Processing test image 12000/13068\n",
            "Task 1: Processing test image 13000/13068\n",
            "\n",
            "Saving Task 1 predictions to pred.json...\n",
            "Task 1 prediction file saved successfully.\n",
            "\n",
            "--- Generating Task 2 predictions (pred.csv) ---\n",
            "Processing Task 1 results for Task 2 (Score Threshold: 0.5)...\n",
            "Task 2: Processed detections for 1000 images...\n",
            "Task 2: Processed detections for 2000 images...\n",
            "Task 2: Processed detections for 3000 images...\n",
            "Task 2: Processed detections for 4000 images...\n",
            "Task 2: Processed detections for 5000 images...\n",
            "Task 2: Processed detections for 6000 images...\n",
            "Task 2: Processed detections for 7000 images...\n",
            "Task 2: Processed detections for 8000 images...\n",
            "Task 2: Processed detections for 9000 images...\n",
            "Task 2: Processed detections for 10000 images...\n",
            "Task 2: Processed detections for 11000 images...\n",
            "Task 2: Processed detections for 12000 images...\n",
            "Task 2 processing finished for 12793 images.\n",
            "Added missing image_id 8717 with prediction -1\n",
            "Added missing image_id 3598 with prediction -1\n",
            "Added missing image_id 11789 with prediction -1\n",
            "Added missing image_id 1040 with prediction -1\n",
            "Added missing image_id 11797 with prediction -1\n",
            "Added missing image_id 10263 with prediction -1\n",
            "Added missing image_id 1063 with prediction -1\n",
            "Added missing image_id 46 with prediction -1\n",
            "Added missing image_id 1082 with prediction -1\n",
            "Added missing image_id 8764 with prediction -1\n",
            "Added missing image_id 11837 with prediction -1\n",
            "Added missing image_id 5185 with prediction -1\n",
            "Added missing image_id 6216 with prediction -1\n",
            "Added missing image_id 11342 with prediction -1\n",
            "Added missing image_id 6735 with prediction -1\n",
            "Added missing image_id 6235 with prediction -1\n",
            "Added missing image_id 8305 with prediction -1\n",
            "Added missing image_id 8316 with prediction -1\n",
            "Added missing image_id 4732 with prediction -1\n",
            "Added missing image_id 8328 with prediction -1\n",
            "Added missing image_id 5257 with prediction -1\n",
            "Added missing image_id 3212 with prediction -1\n",
            "Added missing image_id 1678 with prediction -1\n",
            "Added missing image_id 10383 with prediction -1\n",
            "Added missing image_id 9378 with prediction -1\n",
            "Added missing image_id 7331 with prediction -1\n",
            "Added missing image_id 8869 with prediction -1\n",
            "Added missing image_id 1201 with prediction -1\n",
            "Added missing image_id 11441 with prediction -1\n",
            "Added missing image_id 9411 with prediction -1\n",
            "Added missing image_id 5317 with prediction -1\n",
            "Added missing image_id 711 with prediction -1\n",
            "Added missing image_id 7883 with prediction -1\n",
            "Added missing image_id 6862 with prediction -1\n",
            "Added missing image_id 13008 with prediction -1\n",
            "Added missing image_id 3283 with prediction -1\n",
            "Added missing image_id 4823 with prediction -1\n",
            "Added missing image_id 6884 with prediction -1\n",
            "Added missing image_id 6385 with prediction -1\n",
            "Added missing image_id 7931 with prediction -1\n",
            "Added missing image_id 8957 with prediction -1\n",
            "Added missing image_id 8960 with prediction -1\n",
            "Added missing image_id 1795 with prediction -1\n",
            "Added missing image_id 6403 with prediction -1\n",
            "Added missing image_id 1288 with prediction -1\n",
            "Added missing image_id 280 with prediction -1\n",
            "Added missing image_id 4387 with prediction -1\n",
            "Added missing image_id 4904 with prediction -1\n",
            "Added missing image_id 832 with prediction -1\n",
            "Added missing image_id 6987 with prediction -1\n",
            "Added missing image_id 11093 with prediction -1\n",
            "Added missing image_id 5974 with prediction -1\n",
            "Added missing image_id 4952 with prediction -1\n",
            "Added missing image_id 3929 with prediction -1\n",
            "Added missing image_id 4957 with prediction -1\n",
            "Added missing image_id 9570 with prediction -1\n",
            "Added missing image_id 2920 with prediction -1\n",
            "Added missing image_id 10610 with prediction -1\n",
            "Added missing image_id 5495 with prediction -1\n",
            "Added missing image_id 3449 with prediction -1\n",
            "Added missing image_id 4986 with prediction -1\n",
            "Added missing image_id 7558 with prediction -1\n",
            "Added missing image_id 1930 with prediction -1\n",
            "Added missing image_id 6049 with prediction -1\n",
            "Added missing image_id 5032 with prediction -1\n",
            "Added missing image_id 7081 with prediction -1\n",
            "Added missing image_id 8634 with prediction -1\n",
            "Added missing image_id 7106 with prediction -1\n",
            "Added missing image_id 4547 with prediction -1\n",
            "Added missing image_id 1988 with prediction -1\n",
            "Added missing image_id 10180 with prediction -1\n",
            "Added missing image_id 5581 with prediction -1\n",
            "Added missing image_id 5084 with prediction -1\n",
            "Added missing image_id 12764 with prediction -1\n",
            "Added missing image_id 3554 with prediction -1\n",
            "Added missing image_id 7139 with prediction -1\n",
            "Added missing image_id 10727 with prediction -1\n",
            "Added missing image_id 2543 with prediction -1\n",
            "Added missing image_id 3069 with prediction -1\n",
            "Final total task2 results: 13068 (should be 13068)\n",
            "\n",
            "Saving Task 2 predictions to pred.csv...\n",
            "Task 2 prediction file saved successfully.\n",
            "Script finished.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e76250e4-e153-4a6b-a646-d67f9fa0cd90\", \"pred.json\", 12672377)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a8f71f1e-7a1a-4b29-8df5-e39360341c88\", \"pred.csv\", 121091)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fea64cd2-e493-4e15-ad6c-8b99d9eb2c23\", \"fasterrcnn_svhn_best_model.pth\", 173596114)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "blNAgpD5zuiT",
        "outputId": "db80c4ce-7c85-4417-c41b-44cad5e1e598"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}